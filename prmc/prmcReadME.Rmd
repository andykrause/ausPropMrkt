# Comparison of Price-Rent Ratio Calculation Method

This analysis compares number of methods used to calculate price to rent ratios using a large dataset from Melbourne, Australia.  Unfortunately, our currently licensing on the data does not permit it to be shared.  All code below explained below and held in this repository is free to use, provided you give recognition for doing so. 

The basic order of analysis is to prepared the data, analyze the data, visualize the results and then, finally, assess the predictive quality of the models themselves.  In terms of scripts, the order is as follows:
 
  1. **prmcDataPrep.R**
  2. **prmcDataAnalysis.R**
  3. **prmcDataViz.R**
  4. **prmcPredModels.R**

*(If using raw data from AURIN then data must be built first using the buildAPMData.R file in the root repository)*

\  
&nbsp;

#### Analytical Workflow Diagram

The following diagram shows more details regarding data and overall analytical workflow.

[Diagram](prmcWorkflow.png)

\  
&nbsp;

## Data Preparation

The **prmcDataPrep.R** file handles the data preparation phase of this analysis.  In this context, data preparation refers to the conversion of the raw data (from the source) into the prepared data that is ready for analysis.  

#### Preliminary Commands

The process begin by loading the necessary libraries.

    library(plyr)
    library(dplyr)
    library(reshape2)
    library(stringr)
    library(maptools)
    library(sp)
    library(rgeos)

Next, we source a file containing a set of functions that are used throughout the analysis, **prrFunctions.R**.  This file is sources from it's Github location.  Details of the individual functions contained in this file can be found in the *"Custom Functions"* section at the end of this document. 

     source(paste0('https://raw.githubusercontent.com/andykrause/ausPropMrkt/',
                   'master/prrFunctions.R'))
                   
The final set of preliminary commands set the path to the data as well as the names of the individual files to be loaded.  This analysis requires seven separate files: 1) A files of all sales; 2) a file of rentals; 3) a file contain the space syntax values for all properties (calculated externally in GIS); 4-7) GIS shapefiles of suburb, LGA, SLA1 and post code boundaries. 

    dataPath <- "C:/.../rawData/"
    saleFile <- 'allSales.csv'
    rentFile <- 'allRents.csv'
    ssFile <- 'allSS.csv'
    subGeoFile <- 'Vic_Suburbs.shp'
    lgaGeoFile <- 'Vic_LGAs.shp'
    sla1GeoFile <- 'Vic_SLA1.shp'
    postGeoFile <- 'Vic_PostCodes.shp'
    
### Read in Data

The data is then read into memory using the data path and file names specified above.

     rawSales <- read.csv(paste0(dataPath, saleFile), stringsAsFactors = FALSE)
     rawRents <- read.csv(paste0(dataPath, rentFile), stringsAsFactors = FALSE)
     ssData <- read.csv(paste0(dataPath, ssFile), stringsAsFactors = FALSE)
     subShp <- readShapePoly(paste0(dataPath, subGeoFile))
     lgaShp <- readShapePoly(paste0(dataPath, lgaGeoFile))
     sla1Shp <- readShapePoly(paste0(dataPath, sla1GeoFile))
     postCodeShp <- readShapePoly(paste0(dataPath, postGeoFile))
     
### Data management

The next step is data management.  In this context data management involvees the fixing of data errors and formats, the combination of information from other table and sources, the removal of duplicate observations and the removal of non-essential fields. No observations are removed at this step as these procedures are saved for the *"Data Cleaning"* section that follows.

#### Create unique identifiers

We begin by creating a unique identification number for each sale and rental observation

    rawSales$UID <- paste0('sale', 1:nrow(rawSales))
    rawRents$UID <- paste0('rental', 1:nrow(rawRents))

#### Fix and add fields

Date formats throughout the data are not consistent.  Here we employ the **fixAPMDates()* custom function to standardize all of the data formats.  Again, see the end of the document for explanation of the custom functions.

    rawSales$transDate <- fixAPMDates(rawSales$FinalResultEventDate)
    rawRents$transDate <- fixAPMDates(rawRents$EventDate)

Transaction values for rentals and sales are held in different fields in the raw data. Here they are tranformed into a single, identically names field. 

    rawSales$transValue <- as.numeric(rawSales$FinalResultEventPrice)
    rawRents$transValue <- as.numeric(rawRents$EventPrice)

We then add an identifier as to the type of transaction (prior to combining the two datasets.)

    rawSales$transType <- 'sale'  
    rawRents$transType <- 'rent'

#### Fix latitude and longitude fields

Two separate sets of latitude and longitude values are given, one set for the property itslef and another for the centroid of the street that it fronts.  In a small number of cases no property specific lat/long values are available but street centroid lat/longs are.  To avoid having to filter out the transaction with the missing lat/long values we apply the street centroid value in this small number of cases. 

    sXY <- which(is.na(rawSales$Property_Latitude) | is.na(rawSales$Property_Longitude))
    rXY <- which(is.na(rawRents$Property_Latitude) | is.na(rawRents$Property_Longitude))
    rawSales$Property_Latitude[sXY] <- rawSales$Street_Centroid_Latitude[sXY]
    rawSales$Property_Longitude[sXY] <- rawSales$Street_Centroid_Longitude[sXY]
    rawRents$Property_Latitude[rXY] <- rawRents$Street_Centroid_Latitude[rXY]
    rawRents$Property_Longitude[rXY] <- rawRents$Street_Centroid_Longitude[rXY]

#### Trim fields and combine datasets

Next we remove unnecessary data fields and combine the sales and rental datasets into the **allTrans** object. 

    columnList <- c('UID', 'GeographicalID', 'EventID', 'AddressID', 'FlatNumber', 
                    'transDate', 'transValue', 'transType',
                    'PropertyType', 'Property_Latitude', 'Property_Longitude',
                    'AreaSize', 'Bedrooms', 'Baths', 'Parking','HasFireplace',
                    'HasPool', 'HasGarage', 'HasAirConditioning')
    allTrans <- rbind(rawSales[ ,columnList], rawRents[ ,columnList])

#### Build time specific fields

Here we add a variety of fields dealing with the time of the transaction. First we add the sales year.

    allTrans$transYear <- as.numeric(substr(allTrans$transDate, 1, 4))

Then the month of sale, where June 2010 is equal to month 1

    allTrans$transMonth <- ((12 * (allTrans$transYear - 2010)) + 
                              as.numeric(substr(allTrans$transDate, 6, 7))) - 5
    
Next days, where June 1st 2010 is equal to day 1

    allTrans$transDays <- (as.numeric(allTrans$transDate - as.Date('2010-05-31')))

And, finally, quarter where the third quarter of 2010 is equal to 1 (plus the month of June)

    allTrans$transQtr <- ((allTrans$transMonth - 1) %/% 3) + 1

#### Fix NA in optional fields

For some fields, such as presence of pool or garage, an NA is an acceptable value in that it signifies the lack of the presence of the particular characteristic.  This NA, however, is troublesome in the later analytical steps.  Here we turn these NAs into 0s. 

    naFields <- list('HasPool', 'HasGarage', 'HasAirConditioning', 'HasFireplace')
    for(naF in 1:length(naFields)){
      naX <- which(is.na(allTrans[ ,naFields[[naF]]]))
      allTrans[naX, naFields[[naF]]] <- 0
    }
  
####  Remove duplicates

We check to see if there are duplicate observations, identified here as a property transacting more than once on the same day for the same type of transaction (sale or rental). We create a new field aggregating those three features, remove all observations that are duplicated in that field and then remove the field itself.  

    allTrans$dUID <- paste0(allTrans$AddressID,"..", allTrans$transDate, "..", allTrans$transType)
    allTrans <- subset(allTrans, !duplicated(dUID))
    allTrans$dUID <- NULL  

#### Apply spatial information

Here we apply spatial aggregation information to each observations.  More specifically, using the ``rgeos`` libraries spatial analysis capabilities, we spatially join the LGA, SLA1, Suburb and Postcode location to each observation.  

First we must remove all observations that are missing a latitude and longitude.  Normally an operations requiring the removal of observations (rows) would appear in the data cleaning section that follows, however, due to the necessity of this step prior to assigning spatial information it is undertaken here.

    allTrans <- subset(allTrans, !is.na(Property_Latitude) & !is.na(Property_Longitude))
  
Then we convert to a `SpatialPointsDataFrame`.

    allSP <- SpatialPointsDataFrame(coords=cbind(allTrans$Property_Longitude,
                                               allTrans$Property_Latitude),
                                    data=allTrans)
  
Using the ``over()`` command we then add the postcode designation from the postcode shapefile to the observations.

    spJoin <- over(allSP, postCodeShp)
    allSP@data$postCode <- as.character(spJoin$POA_2006)

Then the suburb, also removing a number of the trailing '- BAL' designations that are found in the shapefile.

    spJoin <- over(allSP, subShp)
    allSP@data$suburb <- as.character(spJoin$NAME_2006)
    allSP@data$suburb <- gsub(' - Bal', '', allSP@data$suburb)
  
Then the SLA1...

    spJoin <- over(allSP, sla1Shp)
    allSP@data$sla1 <- as.character(spJoin$SLA_NAME11)
  
And, finally, the LGA.

    spJoin <- over(allSP, lgaShp)
    allSP@data$lga <- as.character(spJoin$LGA_NAME11)
    
We finish by converting the data back to a non-spatal data frame.    

    allTrans <- allSP@data
    
#### Add space syntax fields

In this step we add the space syntax measurements to the data.  These are measurements of the locations street configurations and overall integration in the larger street network.  Essentially measures of both centrality as well as local network availability.  These metrics are calculated outside of this session in a geographic information system.

The first process here involves trimming out the many possible space syntax metric to one of choice (at the 2500m level) and one of integration (at the 25000m level).

    ssTrim <- ssData[, c('AddressID', 'L_choice_2500', 
                           'T64_Integration_Segment_Length_Wgt_R25000_metric')]
    names(ssTrim)[2:3] <- c('ssChoice', 'ssInteg')

We then attach these value to the transaction observations.

    allTrans$ssChoice <- ssTrim$ssChoice[match(allTrans$AddressID, ssTrim$AddressID)]
    allTrans$ssInteg <- ssTrim$ssInteg[match(allTrans$AddressID, ssTrim$AddressID)]

As temporary objects are large, we clean a number of them from working memory at this juncture.

    rm(rawRents); rm(rawSales); rm(spJoin)
    rm(ssData, ssTrim); gc()

### Data Cleaning

This section deals with the removal of observations from the dataset based on the value in one or more of the field from either the raw data or created/appended above.  

#### Filter by property type

We begin by removing all observation that are not designated as a house or unit (apartment).  This includes terraces, townhomes, villas and duplexes.

    allTrans <- subset(allTrans, PropertyType == 'House' | PropertyType == 'Unit')

#### Missing values

We start by removing all observations with a missing or 0-value in a required field.  Transaction value (sale price or rental amount) is first. 

    allTrans <- subset(allTrans, !is.na(transValue))
    allTrans <- subset(allTrans, transValue  != 0)

Then those observations missing critical home characteristics such as lot size, bedroom or bathroom counts. 

    allTrans <- subset(allTrans, !is.na(AreaSize))
    allTrans <- subset(allTrans, !is.na(Bedrooms))
    allTrans <- subset(allTrans, !is.na(Baths))

And, finally, those missing space syntax values.

    allTrans <- subset(allTrans, !is.na(ssChoice))
    allTrans <- subset(allTrans, !is.na(ssInteg))
  
#### Remove suspect values/outliers
  
In this we remove observation with field values that are either unlikely to be proper values and/or relate to homes that are either extremely small/poor or large/opulent.  These homes do not represent the mass market, trends for which we are interested in estimating.  It is true that these cutoff points are somewhat subjective, however, they have been selected after manually examining the data and choosing points that appropriate either in terms of real world reality (no bathrooms) or natural breaks (such as 8 bedrooms). Note that for bedrooms we have two lower limits.  The first, 0, applies to apartment which could be a studio.  The second, 1, is applied to houses. Also note that there are separate sets of transaction value limits for rentals and sales. Finally, note that rents are quoted by the week (though paid by the month) in Melbourne.   
  
First we set the upper and lower limits. 

    areaLimits <- c(40, 25000)
    bathLimits <- c(1, 8)
    bedLimits <- c(0, 1, 8) 
    rentLimits <- c(125, 2500)
    saleLimits <- c(150000, 4000000)

Then we remove by those limits that apply to all observations (lot size and bathrooms)

    allTrans <- subset(allTrans, AreaSize >= areaLimits[1] & 
                       AreaSize <= areaLimits[2])
    allTrans <- subset(allTrans, Baths >= bathLimits[1] & 
                       Baths <= bathLimits[2])

Next, we remove those units and house that don't meet their respective limits. 

    allTrans <- subset(allTrans, Bedrooms <= bedLimits[3])
    allTransU <- subset(allTrans, PropertyType == 'Unit' & 
                        Bedrooms >= bedLimits[1])
    allTransH <- subset(allTrans, PropertyType == 'House' & 
                        Bedrooms >= bedLimits[2])
    allTrans <- rbind(allTransH, allTransU)

Finally, we split the sales and rentals, apply each transaction value filter and then recombine.

    xSales <- subset(allTrans, transType == 'sale')
    xRentals <- subset(allTrans, transType == 'rent')
    xSales <- subset(xSales, transValue >= saleLimits[1] & 
                       transValue <= saleLimits[2])
    xRentals <- subset(xRentals, transValue >= rentLimits[1] & 
                       transValue <= rentLimits[2])
    allTrans <- rbind(xSales, xRentals)

Again, we clean up the memory

    rm(xSales); rm(xRentals); gc()
    
#### Trim shapefiles to extent of sales

Each of the raw shapefiles for the four geographic aggregations contains all areas for the State of Victoria.  In this step we use the sales and rental observations to limit the shapefiles to the areas covered by the transactional observations. 

First with trim the suburbs

    studySuburbs <- subShp[(which(subShp@data$NAME_2006 %in% 
                             names(table(allTrans$suburb)))), ]
  
Then the post codes

    studyPostCodes <- postCodeShp[(which(postCodeShp@data$POA_2006 %in% 
                                   names(table(allTrans$postCode)))), ]
  
Then the SLA1s

    studySLA1s <- sla1Shp[(which(sla1Shp@data$SLA_NAME11 %in% 
                                  names(table(allTrans$sla1)))), ]
  
And finally the LGAs

    studyLGAs <- lgaShp[(which(lgaShp@data$LGA_NAME11 %in% 
                                  names(table(allTrans$lga)))), ]
  
#### Set space-time limits to geographies

Across all of the future methods used to analyze this data, it is important that sample sizes in each space-time unit of analysis are large enough to limit outlying observations.  In other words, if we are analyzing the median rental yield in a given suburb quarterly that each suburb have at least X number of observation per quarter. These breakdowns also cover the different use-type analyses we intend to do (both uses, house only, unit only and either) We have done this analysis ahead of time and have labeled the transaction such that a field will indicate whether each transaction is valid for each space-time breakdown.  Examples of space-time breakdowns include postcode by year all, suburb by quarter unit only, etc.  A custom function, **prrGeoLimit()** is used to make the calculations.

We begin by making the calculations for all geographies at the year time period

    yearThres <- mapply(prrGeoLimit, 
                        locField=c('postCode', 'sla1', 'suburb', 'lga'), 
                        MoreArgs=list(timeField='transYear',
                                      transData=allTrans,
                                      geoTempLimit=3))
And then add names to the fields

    names(yearThres) <- paste0(rep("YT_"),
                               rep(c('both', 'house', 'unit', 'either'), 4),
                               rep("_", 16),
                               c(rep('postCode',4), rep('sla1',4),
                                 rep('suburb',4), rep('lga', 4)))
  
We then do the same for all geographies quarterly  

    qtrThres <- mapply(prrGeoLimit, 
                       locField=c('postCode', 'sla1', 'suburb', 'lga'), 
                       MoreArgs=list(timeField='transQtr',
                                     transData=allTrans,
                                     geoTempLimit=3))
  
And adding the names

    names(qtrThres) <- paste0(rep("QT_"),
                              rep(c('both', 'house', 'unit', 'either'), 4),
                              rep("_", 16),
                              c(rep('postCode',4), rep('sla1',4),
                              rep('suburb',4), rep('lga', 4))) 
                              
Following this, we use the custom function, **prrApplyThres()** to append the designations to each of the observations.

    allTrans <- prrApplyThres(yearThres[1:4], allTrans, 'YT', 'postCode')
    allTrans <- prrApplyThres(yearThres[5:8], allTrans, 'YT', 'sla1')
    allTrans <- prrApplyThres(yearThres[9:12], allTrans, 'YT', 'suburb')
    allTrans <- prrApplyThres(yearThres[13:16], allTrans, 'YT', 'lga')
    allTrans <- prrApplyThres(qtrThres[1:4], allTrans, 'QT', 'postCode')
    allTrans <- prrApplyThres(qtrThres[5:8], allTrans, 'QT', 'sla1')
    allTrans <- prrApplyThres(qtrThres[9:12], allTrans, 'QT', 'suburb')
    allTrans <- prrApplyThres(qtrThres[13:16], allTrans, 'QT', 'lga')      
    
Then we write the data out as a .csv and the entire working environment as an R workspace.

    save.image(paste0(dataPath, 'cleanData.RData'))
    write.csv(allTrans, paste0(dataPath, 'cleanData.csv'), row.names=F)


\  
&nbsp;



## Data Analysis

The **prmcDataAnalysis.R** file handles the data analysis phase of this analysis.  

#### Preliminary Commands

The process begins by setting a parameter that indicates whether or not to rebuild the data (re-run the **prmcDataPrep.R** file or not)

    reBuildData <- FALSE     

Then we load the necessary libraries.

    library(plyr)
    library(dplyr)
    library(reshape2)
    library(stringr)
    library(maptools)
    library(sp)
    library(rgeos)

Next, we source a file containing a set of functions that are used throughout the analysis, **prrFunctions.R**.  This file is sources from it's Github location.  Details of the individual functions contained in this file can be found in the *"Custom Functions"* section at the end of this document. 

     source(paste0('https://raw.githubusercontent.com/andykrause/ausPropMrkt/',
                   'master/prrFunctions.R'))
                   
We also source an exterior set of function that assist in the data analysis tasks. 

     source(paste0('https://raw.githubusercontent.com/andykrause/',
                   'dataAnalysisTools/master/stShardFunctions.R'))


Finally, we set the path to the data

     dataPath <- "C:/.../rawData/"
     
#### Read in and prepare data

If the **reBuildData** parameter is set to TRUE, then we source the **prmcDataPrep.R** file described above.  If not, then the cleaned data object is loaded into memory.

    if(reBuildData){
       source(paste0('https://raw.githubusercontent.com/andykrause/ausPropMrkt/',
                     'master/akModelComparisonAnalysis/prmcDataPrep.R'))
    } else {
       load(paste0(dataPath, 'cleanData.RData'))
    }
  
### Calculate yield values

In the remainder of this file we calculate the actual yield values for all three methods: 1) Median; 2) Imputation; and 3) Matching.  For the median method, only aggregate values are calculated, for Imputation and Matching first the individual values are computed, then they are aggregated.

#### Median Method

Below we calculate the rental yield values using the median method.  This is accomplished using the **prrStsGeoWrap()** function which is a specialized wrapper function for the more general **spaceTimeShard()** function.  The calculation are done at all five levels and with combined uses and uses (house and unit) separated.

##### Metro Level

    mmMetro <- prrStsGeoWrap(stsData = allTrans,
                             metric=c('transValue', 'transValue'),
                             spaceField='all', timeField='transQtr',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
                              
    mmMetroH <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'House',],
                              metric=c('transValue', 'transValue'),
                              spaceField='all', timeField='transQtr',
                              defDim='time', stsLimit=3, 
                              calcs=list(median='median'))
                      
    mmMetroU <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'Unit',],
                              metric=c('transValue', 'transValue'),
                              spaceField='all', timeField='transQtr',
                              defDim='time', stsLimit=3, 
                              calcs=list(median='median'))
  
##### LGA Level
  
    mmLga<- prrStsGeoWrap(stsData = allTrans,
                          metric=c('transValue', 'transValue'),
                          spaceField='lga', timeField='transQtr',
                          defDim='time', stsLimit=3, 
                          calcs=list(median='median'))
  
    mmLgaH <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'House', ],
                            metric=c('transValue', 'transValue'),
                            spaceField='lga', timeField='transQtr',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
    mmLgaU <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'Unit', ],
                            metric=c('transValue', 'transValue'),
                            spaceField='lga', timeField='transQtr',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
##### SLA1 Level
  
    mmSla <- prrStsGeoWrap(stsData = allTrans,
                           metric=c('transValue', 'transValue'),
                           spaceField='sla1', timeField='transQtr',
                           defDim='time', stsLimit=3, 
                           calcs=list(median='median'))
  
    mmSlaH <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'House', ],
                            metric=c('transValue', 'transValue'),
                            spaceField='sla1', timeField='transQtr',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
    mmSlaU <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'Unit', ],
                            metric=c('transValue', 'transValue'),
                            spaceField='sla1', timeField='transQtr',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
##### Suburb Level
  
    mmSuburb <- prrStsGeoWrap(stsData = allTrans,
                              metric=c('transValue', 'transValue'),
                              spaceField='suburb', timeField='transQtr',
                              defDim='time', stsLimit=3, 
                              calcs=list(median='median'))
  
    mmSuburbH <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'House', ],
                               metric=c('transValue', 'transValue'),
                               spaceField='suburb', timeField='transQtr',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))

    mmSuburbU <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'Unit', ],
                               metric=c('transValue', 'transValue'),
                               spaceField='suburb', timeField='transQtr',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))
  
##### PostCode Level
  
    mmPostcode <- prrStsGeoWrap(stsData = allTrans,
                                metric=c('transValue', 'transValue'),
                                spaceField='postCode', timeField='transQtr',
                                defDim='time', stsLimit=3, 
                                calcs=list(median='median'))
  
    mmPostcodeH <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'House', ],
                                 metric=c('transValue', 'transValue'),
                                 spaceField='postCode', timeField='transQtr',
                                 defDim='time', stsLimit=3, 
                                 calcs=list(median='median'))
    mmPostcodeU <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'Unit', ],
                                 metric=c('transValue', 'transValue'),
                                 spaceField='postCode', timeField='transQtr',
                                 defDim='time', stsLimit=3, 
                                 calcs=list(median='median'))
  
#### Imputation Method

Next we calculate the rental yield values using the imputation method.  First we specify imputation models for both sales price and rental values.  

    regSpecH <- log(transValue) ~ as.factor(postCode) + as.factor(transQtr) + 
                   log(AreaSize) +Bedrooms + Baths + ssInteg + ssChoice
  
    regSpecU <- log(transValue) ~ as.factor(postCode) + as.factor(transQtr) + 
                   Bedrooms + Baths + ssInteg + ssChoice

We then impute the values using the **prrImputeReg()** function.  This function uses the sales model to impute sales prices for the rental observations and vice versa. Results are calculated separately for houses and units.

    houseResults <- prrImputeReg(regSpecH, 
                            subset(allTrans, transType == 'sale' &
                                             PropertyType == 'House' & 
                                             QT_house_postCode == 1),
                            subset(allTrans, transType == 'rent' &
                                             PropertyType == 'House' & 
                                             QT_house_postCode == 1),
                            verbose=TRUE)
  
    unitResults <- prrImputeReg(regSpecU, 
                           subset(allTrans, transType == 'sale' &
                                    PropertyType == 'Unit' & 
                                    QT_unit_postCode == 1),
                           subset(allTrans, transType == 'rent' &
                                    PropertyType == 'Unit' & 
                                    QT_unit_postCode == 1),
                           verbose=TRUE)

We then extract the imputed and observed values and calculated the rental yield

    irValues <- rbind(houseResults$results, unitResults$results)

    irValues$yield <- (irValues$Rent * 52) / irValues$Price

Finally, we add the yield estimates to the original dataset, removing all those for which a yield could not be calculated

    allTrans$yield <- irValues$yield[match(allTrans$UID, irValues$UID)]
    xTrans <- subset(allTrans, !is.na(yield)) 

The calculations are done at all five levels and with combined uses and uses (house and unit) separated.  The generic **spaceTimeShard()** function is used to calculate the median yield value for each quarter.

##### Metro
  
    irMetro <- spaceTimeShard(stsData = xTrans,
                             metric=c('yield'),
                             spaceField='all', timeField='transQtr',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
  
    irMetroH <- spaceTimeShard(xTrans[xTrans$PropertyType == 'House', ],
                               metric=c('yield'),
                               spaceField='all', timeField='transQtr',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))

    irMetroU <- spaceTimeShard(xTrans[xTrans$PropertyType == 'Unit', ],
                               metric=c('yield'),
                               spaceField='all', timeField='transQtr',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))

##### LGA
  
    irLga <- spaceTimeShard(stsData = xTrans,
                            metric=c('yield'),
                            spaceField='lga', timeField='transQtr',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
    irLgaH <- spaceTimeShard(xTrans[xTrans$PropertyType == 'House', ],
                             metric=c('yield'),
                             spaceField='lga', timeField='transQtr',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
  
    irLgaU <- spaceTimeShard(xTrans[xTrans$PropertyType == 'Unit', ],
                             metric=c('yield'),
                             spaceField='lga', timeField='transQtr',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median')) 

##### SLA1
  
    irSla <- spaceTimeShard(stsData = xTrans,
                            metric=c('yield'),
                            spaceField='sla1', timeField='transQtr',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
    irSlaH <- spaceTimeShard(xTrans[xTrans$PropertyType == 'House', ],
                             metric=c('yield'),
                             spaceField='sla1', timeField='transQtr',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
  
    irSlaU <- spaceTimeShard(xTrans[xTrans$PropertyType == 'Unit', ],
                             metric=c('yield'),
                             spaceField='sla1', timeField='transQtr',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))

##### Suburb
  
    irSuburb <- spaceTimeShard(stsData = xTrans,
                               metric=c('yield'),
                               spaceField='suburb', timeField='transQtr',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))
  
    irSuburbH <- spaceTimeShard(xTrans[xTrans$PropertyType == 'House', ],
                                metric=c('yield'),
                                spaceField='suburb', timeField='transQtr',
                                defDim='time', stsLimit=3, 
                                calcs=list(median='median'))
  
    irSuburbU <- spaceTimeShard(xTrans[xTrans$PropertyType == 'Unit', ],
                                metric=c('yield'),
                                spaceField='suburb', timeField='transQtr',
                                defDim='time', stsLimit=3, 
                                calcs=list(median='median'))
  
##### Post Code
  
    irPostcode <- spaceTimeShard(stsData = xTrans,
                                 metric=c('yield'),
                                 spaceField='postCode', timeField='transQtr',
                                 defDim='time', stsLimit=3, 
                                 calcs=list(median='median'))
  
    irPostcodeH <- spaceTimeShard(xTrans[xTrans$PropertyType == 'House', ],
                                  metric=c('yield'),
                                  spaceField='postCode', timeField='transQtr',
                                  defDim='time', stsLimit=3, 
                                  calcs=list(median='median'))
  
    irPostcodeU <- spaceTimeShard(xTrans[xTrans$PropertyType == 'Unit', ],
                                  metric=c('yield'),
                                  spaceField='postCode', timeField='transQtr',
                                  defDim='time', stsLimit=3, 
                                  calcs=list(median='median'))
  
#### Matching Method

Next we calculate the rental yield values using the matching method.  In the first step we extract all of the matching observations (properties that have both rented and sold) using the **prrSaleRentMatch()** function.

     dmData <- prrSaleRentMatch(sales=allTrans[allTrans$transType=='sale',], 
                                rentals=allTrans[allTrans$transType=='rent',],
                                matchField='AddressID', saleField='transValue',
                                rentField='transValue', timeField='transQtr')


The calculations are then done at all five levels and with combined uses and uses (house and unit) separated.  The generic **spaceTimeShard()** function is used to calculate the median yield value for each quarter.

##### Metro

    dmMetro <- spaceTimeShard(stsData = dmData,
                              metric=c('saleYield'),
                              spaceField='all', timeField='saleTime',
                              defDim='time', stsLimit=3, 
                              calcs=list(median='median'))
  
    dmMetroH <- spaceTimeShard(dmData[dmData$PropertyType == 'House',],
                               metric=c('saleYield'),
                               spaceField='all', timeField='saleTime',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))
  
    dmMetroU <- spaceTimeShard(dmData[dmData$PropertyType == 'Unit',],
                               metric=c('saleYield'),
                               spaceField='all', timeField='saleTime',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))
  
##### LGA  
  
    dmLga <- spaceTimeShard(stsData = dmData,
                            metric=c('saleYield'),
                            spaceField='lga', timeField='saleTime',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
    dmLgaH <- spaceTimeShard(dmData[dmData$PropertyType == 'House',],
                             metric=c('saleYield'),
                             spaceField='lga', timeField='saleTime',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
  
    dmLgaU <- spaceTimeShard(dmData[dmData$PropertyType == 'Unit',],
                             metric=c('saleYield'),
                             spaceField='lga', timeField='saleTime',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
  
##### SLA1 
  
    dmSla <- spaceTimeShard(stsData = dmData,
                            metric=c('saleYield'),
                            spaceField='sla1', timeField='saleTime',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
    dmSlaH <- spaceTimeShard(dmData[dmData$PropertyType == 'House',],
                             metric=c('saleYield'),
                             spaceField='sla1', timeField='saleTime',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
  
    dmSlaU <- spaceTimeShard(dmData[dmData$PropertyType == 'Unit',],
                             metric=c('saleYield'),
                             spaceField='sla1', timeField='saleTime',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median')) 

##### Suburb
  
    dmSuburb <- spaceTimeShard(stsData = dmData,
                               metric=c('saleYield'),
                               spaceField='suburb', timeField='saleTime',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))
  
    dmSuburbH <- spaceTimeShard(dmData[dmData$PropertyType == 'House',],
                                metric=c('saleYield'),
                                spaceField='suburb', timeField='saleTime',
                                defDim='time', stsLimit=3, 
                                calcs=list(median='median'))
  
    dmSuburbU <- spaceTimeShard(dmData[dmData$PropertyType == 'Unit',],
                                metric=c('saleYield'),
                                spaceField='suburb', timeField='saleTime',
                                defDim='time', stsLimit=3, 
                                calcs=list(median='median')) 
  
##### PostCode 
  
    dmPostcode <- spaceTimeShard(stsData = dmData,
                                 metric=c('saleYield'),
                                 spaceField='postCode', timeField='saleTime',
                                 defDim='time', stsLimit=3, 
                                 calcs=list(median='median'))
  
    dmPostcodeH <- spaceTimeShard(dmData[dmData$PropertyType == 'House',],
                                  metric=c('saleYield'),
                                  spaceField='postCode', timeField='saleTime',
                                  defDim='time', stsLimit=3, 
                                  calcs=list(median='median'))
  
    dmPostcodeU <- spaceTimeShard(dmData[dmData$PropertyType == 'Unit',],
                                  metric=c('saleYield'),
                                  spaceField='postCode', timeField='saleTime',
                                  defDim='time', stsLimit=3, 
                                  calcs=list(median='median')) 
                                  
### Calculate sale price indices

A number of the vizualizations and analyses that follow require a base price index. In this section we extract the coefficients from the hedonic price models to create use specific as well as a combined price index.
                                  
We begin by extracting the coefficients from the hedonic price models developed in the imputation method

    houseModel <- as.data.frame(houseResults$saleModel$coef)
    unitModel <- as.data.frame(unitResults$saleModel$coef)

    houseTimeCoefs <- c(0, houseModel$Estimate[grep('transQtr', 
                                                    rownames(houseModel))])
    unitTimeCoefs <- c(0, unitModel$Estimate[grep('transQtr', 
                                                    rownames(unitModel))])
  
We then convert these into an index of change, where the first period is the base.  This is done for houses and unit separately.
  
    houseIndex <- c(0, (houseTimeCoefs[-1] - 
                          houseTimeCoefs[-length(houseTimeCoefs)]))
    unitIndex <- c(0, (unitTimeCoefs[-1] - 
                         unitTimeCoefs[-length(unitTimeCoefs)]))
  
We then create a combined index by averaging the two use-specific indices.  
  
    allIndex <- (houseIndex + unitIndex) / 2
  
Finally, we convert this into a list object for flexible use later.

    indexList <- list(all = allIndex,
                      house = houseIndex,
                      unit = unitIndex)
                                    
### Combine results

Before moving on to visualization of the result we first combine all results at the geographic level.  In addition to combining the results into a list object, we also create an aggregated set of results where all three method are combined into singular data.frame object for each of computation (using the **prrAggrGeoData()** function).  This includes the price indices developed above. 

    metroList <- list(mm=list(all=mmMetro, house=mmMetroH, unit=mmMetroU),
                      ir=list(all=irMetro, house=irMetroH, unit=irMetroU),
                      dm=list(all=dmMetro, house=dmMetroH, unit=dmMetroU))
    metroData <- prrAggrGeoData(metroList, indexList)


    lgaList <- list(mm=list(all=mmLga, house=mmLgaH, unit=mmLgaU),
                    ir=list(all=irLga, house=irLgaH, unit=irLgaU),
                    dm=list(all=dmLga, house=dmLgaH, unit=dmLgaU))
    lgaData <- prrAggrGeoData(lgaList, indexList, geoSplit=TRUE)


    slaList <- list(mm=list(all=mmSla, house=mmSlaH, unit=mmSlaU),
                    ir=list(all=irSla, house=irSlaH, unit=irSlaU),
                    dm=list(all=dmSla, house=dmSlaH, unit=dmSlaU))
    slaData <- prrAggrGeoData(slaList, indexList, geoSplit=TRUE)


    postcodeList <- list(mm=list(all=mmPostcode, house=mmPostcodeH, unit=mmPostcodeU),
                         ir=list(all=irPostcode, house=irPostcodeH, unit=irPostcodeU),
                         dm=list(all=dmPostcode, house=dmPostcodeH, unit=dmPostcodeU))
    postcodeData <- prrAggrGeoData(postcodeList, indexList, geoSplit=TRUE)


    suburbList <- list(mm=list(all=mmSuburb, house=mmSuburbH, unit=mmSuburbU),
                       ir=list(all=irSuburb, house=irSuburbH, unit=irSuburbU),
                       dm=list(all=dmSuburb, house=dmSuburbH, unit=dmSuburbU))
      suburbData <- prrAggrGeoData(suburbList, indexList, geoSplit=TRUE)
  
Finally, we save only the necessary objects for use in subsequent analyses.

    save(metroList, metroData, lgaList, lgaData, slaList, slaData,
         postcodeList, postcodeData, suburbList, suburbData, dmData,
         file=paste0(dataPath, 'analysisResults.RData'))
         
\  
&nbsp;

## Data Visualization

The **prmcDataViz.R** file handles the visualization of the results.  

#### Preliminary Commands

We begin by loading the necessary libraries.

    library(plyr)
    library(dplyr)
    library(ggplot2)
    library(reshape2)
    library(stringr)
    library(maptools)
    library(sp)
    library(rgeos)
    library(grid)
    
Next, we set the paths to the data analysis object and to a number of shapefiles for creating maps.  Shapefiles include boundary files for the four different geographic aggregations being analyzed -- suburb, LGA, SLA1 and post code.

    dataPath <- "C:/.../rawData/"
    subGeoFile <- 'Vic_Suburbs.shp'
    lgaGeoFile <- 'Vic_LGAs.shp'
    sla1GeoFile <- 'Vic_SLA1.shp'
    postGeoFile <- 'Vic_PostCodes.shp'

We then set two custom plotting theme for our ggplots; one with a black background for presentation and another with white for publication.

     theme_black <- theme_grey() +
      theme(text = element_text(size=9),
          panel.background = element_rect(colour='black', fill='black'),
          panel.grid.major=element_line(colour='gray20'),
          panel.grid.minor=element_line(colour='gray20'),
          plot.background=element_rect(fill='gray10'),
          axis.title.y=element_text(colour='white'),
          axis.text.y=element_text(hjust=1),
          legend.position='bottom',
          legend.background=element_rect(fill='gray10'),
          legend.key=element_rect(fill='gray10', color='gray10'),
          legend.text=element_text(color='white'),
          legend.title=element_blank())
  

    theme_prr <- theme_grey() +
      theme(text = element_text(size=11),
          panel.background = element_rect(colour='gray95', fill='gray95'),
          panel.grid.major=element_line(colour='white', size=.5),
          panel.grid.minor=element_line(colour='white', size=.1),
          plot.background=element_rect(fill='white'),
          axis.title.y=element_text(colour='black'),
          axis.text.y=element_text(hjust=1),
          legend.position='bottom',
          legend.background=element_rect(fill='white'),
          legend.key=element_rect(fill='white', color='white'),
          legend.text=element_text(color='black'),
          legend.title=element_blank(),
          legend.key.width=unit(2, "cm"),
          strip.background = element_rect(fill = "orange", 
                                          color = "orange", size = .1),
          strip.text.x = element_text(face = "bold"),
          strip.text.y = element_text(face = "bold"))
          
#### Load Data   

In this section we load the necessary data for visualization.  We start by loading in the saved R objects from the data analysis stage.

    load(paste0(dataPath, 'analysisResults.RData'))

Next, the four shapefiles are loaded.

    subShp <- readShapePoly(paste0(dataPath, subGeoFile))
    lgaShp <- readShapePoly(paste0(dataPath, lgaGeoFile))
    sla1Shp <- readShapePoly(paste0(dataPath, sla1GeoFile))
    postCodeShp <- readShapePoly(paste0(dataPath, postGeoFile))
         
#### Data Visualization

Here we begin the data visualization.  We start by setting the three colors (grayscale appropriate), sizes and line types that we'll use in the plotting of the three different methods.

    methCols <- c('navy', 'royalblue2', 'skyblue')
    methSizes <- c(.5, 1.25, 2)
    methLines <- c(1, 1, 1)  

##### Metro Scale

We start with plots at the metro scale. Or, in other words, plots showing analysis of data for the aggregated metropolitan area.

First, a comparison of the three methods with houses and units combined.

    metroPlot <- ggplot(metroData$mix$comp,
                      aes(x=timeName, y=yield, group=method)) + 
               geom_line(aes(colour=method, size=method, linetype=method,
                             lineend='round', linejoin='round')) +
               scale_size_manual(values=methSizes) +
               scale_colour_manual(values=methCols) +
               scale_linetype_manual(values=methLines) + 
               xlab("") + ylab("Rental Yield\n") +
               scale_x_continuous(breaks=seq(2, 18, 4), labels=2011:2015) +
               scale_y_continuous(limits=c(.032, .048),
                                  breaks=seq(.032, .048, .002), 
                                  labels=paste0(format(100 * (seq(.032,
                                                                  .048, .002)),
                                          nsmall=1), "%")) +
               theme_prr

Next we plot the same, but with the results weighted by use

    metroPlotUW <- ggplot(metroData$useWgt$comp, 
                        aes(x=timeName, y=yield, group=method)) + 
                 geom_line(aes(colour=method, size=method, linetype=method,
                               lineend='round', linejoin='round')) +
                 scale_size_manual(values=methSizes) +
                 scale_colour_manual(values=methCols) +
                 scale_linetype_manual(values=methLines) + 
                 xlab("") + ylab("Rental Yield\n") +
                 scale_x_continuous(breaks=seq(2, 18, 4), labels=2011:2015) +
                 scale_y_continuous(limits=c(.032, .048),
                                    breaks=seq(.032, .048, .002), 
                                    labels=paste0(format(100 * 
                                                           (seq(.032, .048,
                                                                .002)),
                                            nsmall=1), "%")) +
                 theme_prr
  
We then separate the analysis by use (house vs. unit).

    metroPlotUse <- ggplot(metroData$use$comp,
                         aes(x=timeName, y=yield, group=method)) + 
                  geom_line(aes(colour=method, size=method, linetype=method,
                                lineend='round', linejoin='round')) +
                  scale_size_manual(values=methSizes) +
                  scale_colour_manual(values=methCols) +
                  scale_linetype_manual(values=methLines) + 
                  xlab("") + ylab("Rental Yield\n") +
                  scale_x_continuous(breaks=seq(2, 18, 4), labels=2011:2015) +
                  scale_y_continuous(limits=c(.030, .050),
                                     breaks=seq(.030, .050, .002), 
                                     labels=paste0(format(100 * (
                                       seq(.030, .050, .002)),
                                            nsmall=1), "%")) +
                  facet_wrap(~use) +
                  theme_prr
  
Next, a plot showing the differences between the three methods against the appreciation rate in the quarter.  This plot shows the differences between the combined house and unit analysis.

    metroDiffPlot_A <- ggplot(metroData$mix$diff, aes(x=pIndex, y=dif)) + 
                     geom_point(colour='black', size=2) + 
                     geom_smooth(method=lm) +
                     xlab("Home Price Movement in Qtr") +
                     ylab("Difference in Rental Yield Estimate\n") +
                     scale_x_continuous(limits=c(-.03, .06),
                                        breaks=seq(-.02, .06, .02), 
                                        labels=paste0(format(100 *
                                                      (seq(-.02, .06, .02)),
                                                       nsmall=1), "%")) +
                     scale_y_continuous(limits=c(0, .011),
                                        breaks=seq(0, .01, .002), 
                                        labels=paste0(format(100 * 
                                                      (seq(0, .01, .002)),
                                                       nsmall=1), "%")) +
                     facet_wrap(~method) +
                     theme_prr

Next, a plot showing the differences between the three methods against the time.  This plot shows the differences between the combined house and unit analysis.

    metroDiffPlot_T <- ggplot(metroData$mix$diff, aes(x=timeName, y=dif)) + 
                            geom_point(colour='black', size=2) + 
                     geom_smooth(method=lm) +
                     xlab("") +
                     ylab("Difference in Rental Yield Estimate\n") +
                     scale_x_continuous(breaks=seq(2, 18, 4), 
                                        labels=2011:2015) +
                     scale_y_continuous(limits=c(0, .011),
                                        breaks=seq(0, .01, .002), 
                                        labels=paste0(format(100 * 
                                                     (seq(0, .01, .002)),
                                                      nsmall=1), "%")) +
                     facet_wrap(~method) +
                     theme_prr
  
A plot showing the differences between the three methods against the appreciation rate in the quarter.  This plot shows the differences between the weigthed house and unit analysis.

    metroDiffUWPlot_A <- ggplot(metroData$useWgt$diff, aes(x=pIndex, y=dif)) + 
                       geom_point(colour='black', size=2) + 
                       geom_smooth(method=lm) +
                       xlab("Home Price Movement in Qtr") +
                       ylab("Difference in Rental Yield Estimate\n") +
                       scale_x_continuous(limits=c(-.03, .06),
                                          breaks=seq(-.02, .06, .02), 
                                          labels=paste0(format(100 *
                                                       (seq(-.02, .06, .02)),
                                                        nsmall=1), "%")) +
                       scale_y_continuous(limits=c(0, .011),
                                          breaks=seq(0, .01, .002), 
                                          labels=paste0(format(100 * 
                                                       (seq(0, .01, .002)),
                                                        nsmall=1), "%")) +
                       facet_wrap(~method) +
                       theme_prr
  
A plot showing the differences between the three methods against time.  This plot shows the differences between the weighted house and unit analysis.

    metroDiffUWPlot_T <- ggplot(metroData$useWgt$diff, aes(x=timeName, y=dif)) + 
                       geom_point(colour='black', size=2) + 
                       geom_smooth(method=lm) +
                       xlab("") +
                       ylab("Difference in Rental Yield Estimate\n") +
                       scale_x_continuous(breaks=seq(2, 18, 4), 
                                          labels=2011:2015) +
                       scale_y_continuous(limits=c(0, .011),
                                          breaks=seq(0, .01, .002), 
                                          labels=paste0(format(100 * 
                                                       (seq(0, .01, .002)),
                                                        nsmall=1), "%")) +
                       facet_wrap(~method) +
                       theme_prr
  
A plot showing the differences between the three methods against the appreciation rate in the quarter.  This plot shows the differences between the separated house and unit analysis.

    metroDiffUPlot_A <- ggplot(metroData$use$diff, aes(x=pIndex, y=dif)) + 
                      geom_point(colour='black', size=2) + 
                      geom_smooth(method=lm) +
                      xlab("Home Price Movement in Qtr") +
                      ylab("Difference in Rental Yield Estimate\n") +
                      scale_x_continuous(limits=c(-.03, .07),
                                         breaks=seq(-.02, .06, .02), 
                                         labels=paste0(format(100 *
                                                      (seq(-.02, .06, .02)),
                                                       nsmall=1), "%")) +
                      scale_y_continuous(limits=c(0, .0098),
                                         breaks=seq(0, .008, .002), 
                                         labels=paste0(format(100 * 
                                                      (seq(0, .008, .002)),
                                                       nsmall=1), "%")) +
                      facet_wrap(~use+method) +
                      theme_prr
                      
A plot showing the differences between the three methods against time.  This plot shows the differences between the separated house and unit analysis.

    metroDiffUPlot_T <- ggplot(metroData$use$diff, aes(x=timeName, y=dif)) + 
                      geom_point(colour='black', size=2) + 
                      geom_smooth(method=lm) +
                      xlab("") + 
                      ylab("Difference in Rental Yield Estimate\n")  +
                      scale_x_continuous(breaks=seq(2, 18, 4), 
                                         labels=2011:2015) +
                      scale_y_continuous(limits=c(-.001, .011),
                                         breaks=seq(0, .01, .002), 
                                         labels=paste0(format(100 * 
                                                      (seq(0, .01, .002)),
                                                       nsmall=1), "%")) +
                      facet_wrap(~use+method) +
                      theme_prr

This plot is an aggregate comparison of all difference between the three methods -- combined, use weighted and use separated.

     metroDiffComp <- ggplot(metroData$mix$diff, aes(x=timeName, y=dif)) + 
                      geom_point(colour='black', size=0) + 
                      geom_smooth(method=lm, se=FALSE, colour='black',
                                  size=1) +
                      geom_smooth(data=metroData$useWgt$diff,
                                  aes(x=timeName, y=dif),
                                  method=lm, se=FALSE,
                                  colour='red', size=1) +
                      geom_smooth(data=metroData$use$diff[1:60,],
                                  aes(x=timeName, y=dif),
                                  method=lm, se=FALSE,
                                  colour='blue', size=1) +
                      geom_smooth(data=metroData$use$diff[61:120,],
                                  aes(x=timeName, y=dif),
                                  method=lm, se=FALSE,
                                  colour='navy', size=1) +
                      xlab("") + 
                      ylab("Difference in Rental Yield Estimate\n")  +
                      scale_x_continuous(breaks=seq(2, 18, 4), 
                                         labels=2011:2015) +
                      scale_y_continuous(limits=c(-.001, .011),
                                         breaks=seq(0, .01, .002), 
                                         labels=paste0(format(100 * 
                                                      (seq(0, .01, .002)),
                                                       nsmall=1), "%")) +
                      facet_wrap(~method) +
                      theme_prr
    
We then build plot showing the differences between the methods at the LGA level.  This plot shows the comparisons of the combined house and unit analysis.

    lgaPlot <- ggplot(lgaData$mix$comp, 
                    aes(x=timeName, y=yield, group=spaceName)) + 
    geom_line(colour='gray50', size= 0.1, lineend='round', linejoin='round') +
    geom_line(data=lgaData$mixWgt$comp, 
              aes(x=timeName, y=yield),
              colour='black', size=1.5, lineend='round', linejoin='round') +
    xlab("") + ylab("Rental Yield\n") +
    scale_x_continuous(breaks=seq(2, 18, 4), labels=2011:2015) +
    scale_y_continuous(limits=c(.015, .059),
                       breaks=seq(.015, .059, .004), 
                       labels=paste0(format(100 * (seq(.015, .059, .004)),
                                            nsmall=1), "%")) +
    facet_wrap(~method)+
    theme_prr
  
We then plot the differences versus time (again, combined house and unit analysis) at the LGA level.
  
    lgaDiffComp_T <- ggplot(lgaData$mix$diff, aes(x=timeName, y=dif, 
                                              group=spaceName)) + 
                 geom_point(colour='black', size=0) + 
                 stat_smooth(data=lgaData$mix$diff,
                             aes(x=timeName, y=dif, group=spaceName),
                             method=loess, se=FALSE, colour='gray50') +
    stat_smooth(data=lgaData$mixWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=TRUE,
                colour='black', size=2) +
    stat_smooth(data=lgaData$useWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=TRUE,
                colour='blue', size=2) +
    xlab("") + 
    ylab("Difference in Rental Yield Estimate\n")  +
    facet_wrap(~method) +
    theme_prr


We then plot the differences versus quarterly appreciation (again, combined house and unit analysis) at the LGA level.

    lgaDiffComp_A <- ggplot(lgaData$mix$diff, aes(x=pIndex, y=dif, 
                                                group=spaceName)) + 
    geom_point(colour='black', size=0) + 
    stat_smooth(data=lgaData$mix$diff,
                aes(x=pIndex, y=dif, group=spaceName),
                method=loess, se=FALSE, colour='gray50') +
    stat_smooth(data=lgaData$mixWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=TRUE,
                colour='black', size=2) +
    stat_smooth(data=lgaData$useWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=TRUE,
                colour='blue', size=2) +
    xlab("") + 
    ylab("Difference in Rental Yield Estimate\n")  +
    facet_wrap(~method) +
    theme_prr
  
We then build plot showing the differences between the methods at the suburb level.  This plot shows the comparisons of the combined house and unit analysis.

    suburbPlot <- ggplot(suburbData$mix$comp, aes(x=timeName, y=yield, 
                                          group=spaceName)) + 
    geom_line(colour='gray50', size= 0.1, lineend='round', linejoin='round') +
    geom_line(data=suburbData$mixWgt$comp, 
              aes(x=timeName, y=yield),
              colour='black', size=1.5, lineend='round', linejoin='round') +
    xlab("") + ylab("Rental Yield\n") +
    scale_x_continuous(breaks=seq(2, 18, 4), labels=2011:2015) +
    scale_y_continuous(limits=c(.01, .079),
                       breaks=seq(.015, .059, .004), 
                       labels=paste0(format(100 * (seq(.015,
                                                       .059, .004)),
                                            nsmall=1), "%")) +
    facet_wrap(~method)+
    theme_prr
  
We then plot the differences versus time (again, combined house and unit analysis) at the suburb level.

    suburbDiffComp_T <- ggplot(suburbData$mix$diff, aes(x=timeName, y=dif, 
                                              group=spaceName)) + 
    geom_point(colour='black', size=0) + 
    stat_smooth(data=suburbData$mix$diff,
                aes(x=timeName, y=dif, group=spaceName),
                method=loess, se=FALSE, colour='gray50') +
    stat_smooth(data=suburbData$mixWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='black', size=2) +
    stat_smooth(data=suburbData$useWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='blue', size=2) +
    xlab("") + 
    ylab("Difference in Rental Yield Estimate\n")  +
    scale_y_continuous(limits=c(-.001, .011),
                       breaks=seq(0, .01, .002), 
                       labels=paste0(format(100 * (seq(0, .01, .002)),
                                                 nsmall=1), "%")) +
    facet_wrap(~method) +
    theme_prr
  

We then plot the differences versus quarterly appreciation (again, combined house and unit analysis) at the suburb level.

     suburbDiffComp_A <- ggplot(suburbData$mix$diff, 
                             aes(x=pIndex, y=dif, group=spaceName)) + 
    geom_point(colour='black', size=0) + 
    stat_smooth(data=suburbData$mix$diff,
                aes(x=pIndex, y=dif, group=spaceName),
                method=loess, se=FALSE, colour='gray50') +
    stat_smooth(data=suburbData$mixWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='black', size=2) +
    stat_smooth(data=suburbData$useWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='blue', size=2) +
    xlab("") + 
    ylab("Difference in Rental Yield Estimate\n")  +
    scale_y_continuous(limits=c(-.001, .011),
                       breaks=seq(0, .01, .002), 
                       labels=paste0(format(100 * (seq(0, .01, .002)),
                                            nsmall=1), "%")) +
    facet_wrap(~method) +
    theme_prr
  
Finally, we plot all differnces against time for the metro, LGA and suburb analyses.

    allDiffComp_T <- ggplot(lgaData$mix$diff, aes(x=timeName, y=dif, 
                                                    group=spaceName)) + 
                 geom_point(colour='black', size=0) + 
    stat_smooth(data=suburbData$mixWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='black', size=2, linetype=2) +
    stat_smooth(data=lgaData$mixWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='black', size=2) +
    stat_smooth(data=lgaData$useWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='blue', size=2) +
    stat_smooth(data=metroData$mix$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='red', size=2) +
    stat_smooth(data=metroData$useWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='red', size=2, linetype=2) +
    stat_smooth(data=suburbData$useWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='blue', size=2, linetype=2) +
    xlab("") + 
    ylab("Difference in Rental Yield Estimate\n")  +
    scale_y_continuous(limits=c(-.001, .011),
                       breaks=seq(0, .01, .002), 
                       labels=paste0(format(100 * 
                                              (seq(0, .01, .002)),
                                            nsmall=1), "%")) +
    facet_wrap(~method) +
    theme_prr
 
Finally, we plot all differences against quarterly appreciation for the metro, LGA and suburb analyses.

    allDiffComp_A <- ggplot(lgaData$mix$diff, aes(x=pIndex, y=dif, 
                                                group=spaceName)) + 
    geom_point(colour='black', size=0) + 
    stat_smooth(data=suburbData$mixWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='black', size=2) +
    stat_smooth(data=lgaData$mixWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='black', size=2, linetype=2) +
    stat_smooth(data=lgaData$useWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='blue', size=2) +
    stat_smooth(data=metroData$mix$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='red', size=2) +
    stat_smooth(data=metroData$useWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='red', size=2, linetype=2) +
    stat_smooth(data=suburbData$useWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='blue', size=2, linetype=2) +
    xlab("") + 
    ylab("Difference in Rental Yield Estimate\n")  +
    scale_y_continuous(limits=c(0, .0105),
                       breaks=seq(0, .008, .002), 
                       labels=paste0(format(100 * (seq(0, .008, .002)),
                                                nsmall=1), "%")) +
    facet_wrap(~method) +
    theme_prr
    
\  
&nbsp;

## Predictive Modeling

The **prmcPredModels.R** file contains code where we test the predictive ability of the three methods across the five different spatial aggregation levels.

#### Preliminary Commands 

Per the usual, we begin by loading the necessary libraries.

    library(plyr)
    library(dplyr)
    library(ggplot2)
    library(reshape2)
    library(stringr)
    library(maptools)
    library(sp)
    library(rgeos)
    library(grid)

We then source the necessary custom functions (directly from Github)

    source(paste0('https://raw.githubusercontent.com/andykrause/ausPropMrkt/',
                  'master/prrFunctions.R'))
  
#### Load Data   

Next we load the R objects from the data analysis phase.  First we set the path to the data.

    dataPath <- "C:/.../rawData/"

Then load the objects.

    load(paste0(dataPath, 'analysisResults.RData'))
    
#### Extract necessary data 
  
Next we convert the analytical data objects into data objects that are useful for predictive modeling.  To do so, we use the **prrGetYields()** custom function.

    metroYields <- prrGetYields(metroData)
    lgaYields <- prrGetYields(lgaData)
    slaYields <- prrGetYields(slaData)
    postcodeYields <- prrGetYields(postcodeData)
    suburbYields <- prrGetYields(suburbData)

We also add a unique identifier to the direct match data at this step

    dmData$uID <- 1:nrow(dmData)

#### Estimate prediction errors 
  
We begin by estimating the raw prediction errors, where prediction error is the difference between the actual price to rent ratio for each of the matched observations versus that predicted by the three estimation methods.  We use the custom function **prrPredModelWrap** to complete this operation.

Starting with the metro level, combined and use separated (*byUse=TRUE*)

    metroRes <- prrPredModelWrap(dmData, metroYields)
    metroResU <- prrPredModelWrap(dmData, metroYields, byUse=TRUE)
  
Then at the LGA level

    lgaRes <- prrPredModelWrap(dmData, lgaYields, byGeog=TRUE, geoField='lga')
    lgaResU <- prrPredModelWrap(dmData, lgaYields, byGeog=TRUE, 
                                geoField='lga', byUse=TRUE)
  
And, then at the suburb level

    suburbRes <- prrPredModelWrap(dmData, suburbYields, byGeog=TRUE, 
                                  geoField='suburb')
    suburbResU <- prrPredModelWrap(dmData, suburbYields, byGeog=TRUE, 
                                   geoField='suburb', byUse=TRUE)


#### Attached Prediction Errors
                                   
Next, we attach the individual prediction errors from each of the three methods and the two use categories to the individual observations in the matched dataset.  This is done for all three levels of analysis.

At the metro level

    dmData$mMed <- metroRes$median$error[match(dmData$uID, metroRes$median$uID)]
    dmData$mMedU <- metroResU$median$error[match(dmData$uID, metroResU$median$uID)]
    dmData$mImp <- metroRes$impute$error[match(dmData$uID, metroRes$impute$uID)]
    dmData$mImpU <- metroResU$impute$error[match(dmData$uID, metroResU$impute$uID)]
    dmData$mMat <- metroRes$match$error[match(dmData$uID, metroRes$match$uID)]
    dmData$mMatU <- metroResU$match$error[match(dmData$uID, metroResU$match$uID)]

The LGA level

    dmData$lMed <- lgaRes$median$error[match(dmData$uID, lgaRes$median$uID)]
    dmData$lMedU <- lgaResU$median$error[match(dmData$uID, lgaResU$median$uID)]
    dmData$lImp <- lgaRes$impute$error[match(dmData$uID, lgaRes$impute$uID)]
    dmData$lImpU <- lgaResU$impute$error[match(dmData$uID, lgaResU$impute$uID)]
    dmData$lMat <- lgaRes$match$error[match(dmData$uID, lgaRes$match$uID)]
    dmData$lMatU <- lgaResU$match$error[match(dmData$uID, lgaResU$match$uID)]

The suburb level

    dmData$sMed <- suburbRes$median$error[match(dmData$uID, suburbRes$median$uID)]
    dmData$sMedU <- suburbResU$median$error[match(dmData$uID, suburbResU$median$uID)]
    dmData$sImp <- suburbRes$impute$error[match(dmData$uID, suburbRes$impute$uID)]
    dmData$sImpU <- suburbResU$impute$error[match(dmData$uID, suburbResU$impute$uID)]
    dmData$sMat <- suburbRes$match$error[match(dmData$uID, suburbRes$match$uID)]
    dmData$sMatU <- suburbResU$match$error[match(dmData$uID, suburbResU$match$uID)]

#### Calculate Prediction Error

Before calculation, we first create an object contain the absolute value of the individual prediction errors.

    absPredResults <- lapply(dmData[ ,(which(colnames(dmData) == 'mMed'):
                                        which(colnames(dmData) == 'sMatU'))], abs)

We then calculate the median of each of the 18 predictions (3 methods x 2 use types x 3 geographic levels)

    absPredMed <- lapply(absPredResults, median, na.rm=TRUE)

The results are then placed in a 6 x 3 table.

    absPredMed <- as.matrix(unlist(absPredMed))
    predTable <- data.frame(metro=absPredMed[1:6],
                            lga=absPredMed[7:12],
                            suburb=absPredMed[13:18])
    rownames(predTable) <- c('Median', 'Median by Use', 'Impute', 'Impute by Use',
                             'Match', 'Match by Use')

#### Calculate Hit Rate

We also calculate the hit rate, or the percentage of total observations that each methods is able to give a predicted value on.  

First we build a simple function to count the number of non-predicted observations in each method.

    countNA <- function(x){length(which(is.na(x)))/length(x)}

We then calculate the hit rate for each of the 18 predictions

    hitRate <- lapply(absPredResults, countNA)

The results are then placed in a 6 x 3 table

    hitRate <- 1 - as.matrix(unlist(hitRate))
    hrTable <- data.frame(metro=hitRate[1:6],
                          lga=hitRate[7:12],
                          suburb=hitRate[13:18])
    rownames(hrTable) <- c('Median', 'Median by Use', 'Impute', 'Impute by Use',
                           'Match', 'Match by Use')

#### Save workspace 

Finally we save the workspace.

    save(dmData, predTable, hrTable, 
         file=paste0(dataPath, 'predModelResults.RData'))
         
    
\  
&nbsp;
\  
&nbsp;
\  
&nbsp;
\  
&nbsp;

## Custom Functions ----------------------------------------------------------------------

Here we describe in detail the functionality of all of the custom functions used in this analysis. the code for these functions is found in the **prrFunctions.R** file.

### fixAPMDates()

This function converts the various date formats in the APM data to the standard R format.

This function takes 1 argument:

1. **xDates**: A vector of incorrectly formatted date objects from APM

Declare the function

    fixAPMDates <- function(xDates){
  
Load required libraries

    require(stringr)

Remove any time stamps from the dates

    xDates <- gsub(" 0:00", "", xDates)

Find the character location of the date separators (slashes)
  
    sLoc <- matrix(unlist(str_locate_all(xDates, '/')), ncol=4, byrow=TRUE)[,1:2]
  
Extract and correct the days component (if it is missing a leading 0)

    days <- as.numeric(substr(xDates, 1, sLoc[ ,1] - 1))
    days <- ifelse(days < 10, paste0('0', days), as.character(days))
    
Extract and correct the months component (if it is missing a leading 0)

    months <- as.numeric(substr(xDates, sLoc[ ,1] + 1, sLoc[ ,2] - 1))
    months <- ifelse(months < 10, paste0('0', months), as.character(months))
  
Extract and correct the years component (if it is missing a leading 20)

    years <- as.numeric(substr(xDates, sLoc[ ,2] + 1, 50))
    years <- ifelse(years < 2000, paste0('20', years), as.character(years))

Recombine the three components into the R date format

    newDates <- as.Date(paste0(days, '/' , months, '/', years), "%d/%m/%Y")

Return value to the function call

    return(newDates)
    }

\  
&nbsp;

### prrImputeReg()

This function uses linear regression to create imputed rent and sales values.

This function takes 4 arguments:

1. **formula**: The linear regression formula used to create rent and price estimates
2. **saleData**: A data frame of sales observations
3. **rentData**: A data frame of rental observations
4. **verbose**: (FALSE)  Should progress be reported to the console?

Declare the function

    prrImputeReg <- function(formula, saleData, rentData, verbose = FALSE){
  
Fit the sale and rent models

    if(verbose) cat('Estimating sale and rent models\n')
    saleModel <- lm(formula, data=saleData)
    rentModel <- lm(formula, data=rentData)

Use model to predict rents for sale observations and vice versa

    if(verbose) cat('Imputing values\n')
    impPrice <- exp(predict(saleModel, newdata=rentData))
    impRent <- exp(predict(rentModel, newdata=saleData))
  
Add the imputed values to the original data frames

    if(verbose) cat('Stacking observed and imputed values\n')
    saleData$Price <- saleData$transValue
    rentData$Price <- impPrice
    saleData$Rent <- impRent
    rentData$Rent <- rentData$transValue
  
Merge the sales and rent data into a single data frame

    if(verbose) cat('Merging data\n')
    allData <- rbind(saleData, rentData)

Extract the model coefficient, diagnostics and residuals.  Place in a list object for the rent and the sales model.

    saleModelInfo <- list(coef=summary(saleModel)$coefficients,
                        r2=summary(saleModel)$r.squared,
                        sigma=summary(saleModel)$r.squared,
                        resid=summary(saleModel)$residuals)
    rentModelInfo <- list(coef=summary(rentModel)$coefficients,
                        r2=summary(rentModel)$r.squared,
                        sigma=summary(rentModel)$r.squared,
                        resid=summary(rentModel)$residuals)
  
Return the results in a list object.  First the observation ID with Price and Rent, then the two list of model information.  
  
    return(list(results = allData[ ,c('UID', 'Price', 'Rent')],
                saleModel = saleModelInfo,
                rentModel = rentModelInfo))
         
    }

\  
&nbsp;

### prrGeoLimit()

This function takes a set of transactional data and a geographic field and determines which geographic areas have enough observations in them per the selected time breakdown.

This function takes 4 arguments:

1. **transData**: The data frame of the transactions
2. **locField**: ('locName') The name of the field with the location in it. 
3. **timeField**: ('transYear') The name of the field with the temporal indicator in it
4. **geoTempLimit**: (3) How many observations are required in each location at each time

Declare the function

    prrGeoLimit <- function(transData, locField = 'locName', timeField = 'transYear', 
                            geoTempLimit = 3){  
  
Split the transaction by use and by transaction type

    houseSales <- subset(transData, PropertyType == 'House' &
                           transType == 'sale')
    unitSales <- subset(transData, PropertyType == 'Unit' & 
                          transType == 'sale')
    houseRentals <- subset(transData, PropertyType == 'House' & 
                             transType == 'rent')
    unitRentals <- subset(transData, PropertyType == 'Unit' & 
                            transType == 'rent')
  
Determine which areas meet the criteria for each of the use/transaction type permutations.  This process creates a list of geographic indicators that meet the criteria in the **xxGeo** objects.

House Sales

    saleHTable <- table(houseSales[,locField], houseSales[,timeField])
    shKeep <- which(apply(saleHTable, 1, min) >= geoTempLimit)
    shGeo <- rownames(saleHTable[shKeep, ])

Unit Sales

    saleUTable <- table(unitSales[,locField], unitSales[,timeField])
    suKeep <- which(apply(saleUTable, 1, min) >= geoTempLimit)
    suGeo <- rownames(saleUTable[suKeep, ])

House Rentals

    rentHTable <- table(houseRentals[,locField], houseRentals[,timeField])
    rhKeep <- which(apply(rentHTable, 1, min) >= geoTempLimit)
    rhGeo <- rownames(rentHTable[rhKeep, ])

Unit Rentals

    rentUTable <- table(unitRentals[,locField], unitRentals[,timeField])
    ruKeep <- which(apply(rentUTable, 1, min) >= geoTempLimit)
    ruGeo <- rownames(rentUTable[ruKeep, ])
  
Intersect all accepatable lists of geographies to find those that meet the criteria in all four cases, as well as for just houses and units (and either)

    bothGeo <- intersect(intersect(intersect(shGeo, suGeo), rhGeo), ruGeo)
    houseGeo <- intersect(shGeo,rhGeo)
    unitGeo <- intersect(suGeo, ruGeo)
    eitherGeo <- union(houseGeo, unitGeo)

Return a list of acceptable geographies in all four situations.

    return(list(bothGeo = bothGeo,
                houseGeo = houseGeo,
                unitGeo = unitGeo,
                eitherGeo = eitherGeo))  
    }
